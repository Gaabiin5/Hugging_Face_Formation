{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gaabiin5/Hugging_Face_Formation/blob/main/agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5jysBiKyEw8",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "# Agents in LlamaIndex\n",
        "\n",
        "This notebook is part of the [Hugging Face Agents Course](https://www.hf.co/learn/agents-course), a free Course from beginner to expert, where you learn to build Agents.\n",
        "\n",
        "![Agents course share](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/communication/share.png)\n",
        "\n",
        "## Let's install the dependencies\n",
        "\n",
        "We will install the dependencies for this unit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rQoSlQVryEw_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install llama-index llama-index-vector-stores-chroma llama-index-llms-huggingface-api llama-index-embeddings-huggingface -U -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGya0sxhyExB"
      },
      "source": [
        "And, let's log in to Hugging Face to use serverless Inference APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ix2ni1oFyExC"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4a52775f9304df78d435bdc0db5bcff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3BlhAa9yExD",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "## Initialising agents\n",
        "\n",
        "Let's start by initialising an agent. We will use the basic `AgentWorkflow` class to create an agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: llama-index in /home/gan/venv/lib/python3.12/site-packages (0.12.43)\n",
            "Requirement already satisfied: llama-index-llms-ollama in /home/gan/venv/lib/python3.12/site-packages (0.6.2)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5,>=0.4.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index) (0.4.11)\n",
            "Requirement already satisfied: llama-index-cli<0.5,>=0.4.2 in /home/gan/venv/lib/python3.12/site-packages (from llama-index) (0.4.3)\n",
            "Requirement already satisfied: llama-index-core<0.13,>=0.12.43 in /home/gan/venv/lib/python3.12/site-packages (from llama-index) (0.12.43)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4,>=0.3.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index) (0.7.7)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.5,>=0.4.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index) (0.4.7)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.6,>=0.5.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index) (0.5.1)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4,>=0.3.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index) (0.3.2)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4,>=0.3.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-readers-file<0.5,>=0.4.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index) (0.4.9)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index) (0.4.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /home/gan/venv/lib/python3.12/site-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: ollama>=0.5.1 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-llms-ollama) (0.5.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.88.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (3.12.13)\n",
            "Requirement already satisfied: aiosqlite in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (2.1.2)\n",
            "Requirement already satisfied: dataclasses-json in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (2025.3.0)\n",
            "Requirement already satisfied: httpx in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows>=0.2.1 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (0.2.2)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (3.5)\n",
            "Requirement already satisfied: numpy in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (2.3.0)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (11.2.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (2.11.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /home/gan/venv/lib/python3.12/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.43->llama-index) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (9.1.2)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (4.14.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.43->llama-index) (1.17.2)\n",
            "Requirement already satisfied: llama-cloud==0.1.26 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.26)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /home/gan/venv/lib/python3.12/site-packages (from llama-cloud==0.1.26->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.6.15)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (4.13.4)\n",
            "Requirement already satisfied: pandas<2.3.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.2.3)\n",
            "Requirement already satisfied: pypdf<6,>=5.1.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (5.6.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.34)\n",
            "Requirement already satisfied: click in /home/gan/venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index) (8.2.1)\n",
            "Requirement already satisfied: joblib in /home/gan/venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/gan/venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/gan/venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.43->llama-index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/gan/venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.43->llama-index) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/gan/venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.43->llama-index) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/gan/venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.43->llama-index) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/gan/venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.43->llama-index) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/gan/venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.43->llama-index) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/gan/venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.43->llama-index) (1.20.1)\n",
            "Requirement already satisfied: griffe in /home/gan/venv/lib/python3.12/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.43->llama-index) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /home/gan/venv/lib/python3.12/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.43->llama-index) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in /home/gan/venv/lib/python3.12/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.43->llama-index) (4.3.8)\n",
            "Requirement already satisfied: soupsieve>1.2 in /home/gan/venv/lib/python3.12/site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.7)\n",
            "Requirement already satisfied: anyio in /home/gan/venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.13,>=0.12.43->llama-index) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /home/gan/venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.13,>=0.12.43->llama-index) (1.0.9)\n",
            "Requirement already satisfied: idna in /home/gan/venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.13,>=0.12.43->llama-index) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /home/gan/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.43->llama-index) (0.16.0)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-workflows>=0.2.1->llama-index-core<0.13,>=0.12.43->llama-index) (0.1.0)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.32 in /home/gan/venv/lib/python3.12/site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.34)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/gan/venv/lib/python3.12/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /home/gan/venv/lib/python3.12/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /home/gan/venv/lib/python3.12/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gan/venv/lib/python3.12/site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/gan/venv/lib/python3.12/site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/gan/venv/lib/python3.12/site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/gan/venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.43->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /home/gan/venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.43->llama-index) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /home/gan/venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.43->llama-index) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/gan/venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.43->llama-index) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gan/venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.43->llama-index) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /home/gan/venv/lib/python3.12/site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.43->llama-index) (3.2.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/gan/venv/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.43->llama-index) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/gan/venv/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.43->llama-index) (3.26.1)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /home/gan/venv/lib/python3.12/site-packages (from llama-cloud-services>=0.6.32->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.1.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /home/gan/venv/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13,>=0.12.43->llama-index) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/gan/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (1.17.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /home/gan/venv/lib/python3.12/site-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.43->llama-index) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/gan/venv/lib/python3.12/site-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.43->llama-index) (3.0.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install llama-index llama-index-llms-ollama\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZjIj2hQayExK"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.core.agent.workflow import AgentWorkflow, ToolCallResult, AgentStream\n",
        "\n",
        "\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two numbers\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "def subtract(a: int, b: int) -> int:\n",
        "    \"\"\"Subtract two numbers\"\"\"\n",
        "    return a - b\n",
        "\n",
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two numbers\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "def divide(a: int, b: int) -> int:\n",
        "    \"\"\"Divide two numbers\"\"\"\n",
        "    return a / b\n",
        "\n",
        "\n",
        "# llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\") # Online with credits\n",
        "llm = Ollama(model=\"mistral:instruct\")  # Modele local | Open chat does not support tools\n",
        "\n",
        "agent = AgentWorkflow.from_tools_or_functions(\n",
        "    tools_or_functions=[subtract, multiply, divide, add],\n",
        "    llm=llm,\n",
        "    system_prompt=\"You are a math agent that can add, subtract, multiply, and divide numbers using provided tools.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z04Sb59uyExM"
      },
      "source": [
        "Then, we can run the agent and get the response and reasoning behind the tool calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "QIx1q-2IyExN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " To solve the equation (2 + 2) * 2, first calculate the addition part:\n",
            "\n",
            "2 + 2 = 4\n",
            "\n",
            "Then perform multiplication with the result:\n",
            "\n",
            "4 * 2 = 8\n",
            "\n",
            "So, the answer to (2 + 2) * 2 is 8."
          ]
        },
        {
          "data": {
            "text/plain": [
              "AgentOutput(response=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={'tool_calls': [], 'thinking': ''}, blocks=[TextBlock(block_type='text', text=' To solve the equation (2 + 2) * 2, first calculate the addition part:\\n\\n2 + 2 = 4\\n\\nThen perform multiplication with the result:\\n\\n4 * 2 = 8\\n\\nSo, the answer to (2 + 2) * 2 is 8.')]), tool_calls=[], raw={'model': 'mistral:instruct', 'created_at': '2025-06-20T10:45:53.886384987Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2862087752, 'load_duration': 7376102, 'prompt_eval_count': 374, 'prompt_eval_duration': 494868017, 'eval_count': 69, 'eval_duration': 2357998345, 'message': Message(role='assistant', content='', thinking=None, images=None, tool_calls=None), 'usage': {'prompt_tokens': 374, 'completion_tokens': 69, 'total_tokens': 443}}, current_agent_name='Agent')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "handler = agent.run(\"What is (2 + 2) * 2?\")\n",
        "async for ev in handler.stream_events():\n",
        "    if isinstance(ev, ToolCallResult):\n",
        "        print(\"\")\n",
        "        print(\"Called tool: \", ev.tool_name, ev.tool_kwargs, \"=>\", ev.tool_output)\n",
        "    elif isinstance(ev, AgentStream):  # showing the thought process\n",
        "        print(ev.delta, end=\"\", flush=True)\n",
        "\n",
        "resp = await handler\n",
        "resp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9ZvR7h0yExO"
      },
      "source": [
        "In a similar fashion, we can pass state and context to the agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mt9aZ73syExO",
        "outputId": "1c1771bf-9283-4ded-ad9f-068ceb7c49dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AgentOutput(response=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={'tool_calls': [], 'thinking': ''}, blocks=[TextBlock(block_type='text', text='Your name is Bob.')]), tool_calls=[], raw={'model': 'phi4-mini:latest', 'created_at': '2025-06-20T10:23:24.831562188Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1358864186, 'load_duration': 57158906, 'prompt_eval_count': 330, 'prompt_eval_duration': 470721656, 'eval_count': 6, 'eval_duration': 799011395, 'message': Message(role='assistant', content='', thinking=None, images=None, tool_calls=None), 'usage': {'prompt_tokens': 330, 'completion_tokens': 6, 'total_tokens': 336}}, current_agent_name='Agent')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from llama_index.core.workflow import Context\n",
        "\n",
        "ctx = Context(agent)\n",
        "\n",
        "response = await agent.run(\"My name is Bob.\", ctx=ctx)\n",
        "response = await agent.run(\"What was my name again?\", ctx=ctx)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvhqHLBEyExP"
      },
      "source": [
        "## Creating RAG Agents with QueryEngineTools\n",
        "\n",
        "Let's now re-use the `QueryEngine` we defined in the [previous unit on tools](/tools.ipynb) and convert it into a `QueryEngineTool`. We will pass it to the `AgentWorkflow` class to create a RAG agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pCI3Um9DyExQ"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "# Create a vector store\n",
        "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
        "chroma_collection = db.get_or_create_collection(\"alfred\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "# Create a query engine\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
        "llm = Ollama(model=\"mistral:instruct\")  # Modele local\n",
        "\n",
        "index = VectorStoreIndex.from_vector_store(\n",
        "    vector_store=vector_store, embed_model=embed_model\n",
        ")\n",
        "query_engine = index.as_query_engine(llm=llm)\n",
        "query_engine_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=query_engine,\n",
        "    name=\"personas\",\n",
        "    description=\"descriptions for various types of personas\",\n",
        "    return_direct=True, # False = can ignore the tool\n",
        ")\n",
        "\n",
        "# Create a RAG agent\n",
        "query_engine_agent = AgentWorkflow.from_tools_or_functions(\n",
        "    tools_or_functions=[query_engine_tool],\n",
        "    llm=llm,\n",
        "    system_prompt=\"You are an assistant with access to a persona database via a tool called `personas`. You must always use the tool to answer queries. Do not invent or guess answers.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmeZDsdeyExR"
      },
      "source": [
        "And, we can once more get the response and reasoning behind the tool calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bRZ4lIFJyExR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Called tool:  personas {'input': 'science fiction'} =>  It seems neither persona in the provided context has a particular interest or expertise in science fiction. They focus on Cypriot culture and history for the first persona and 19th-century American art and local cultural heritage of Cincinnati for the second persona.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AgentOutput(response=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text=' It seems neither persona in the provided context has a particular interest or expertise in science fiction. They focus on Cypriot culture and history for the first persona and 19th-century American art and local cultural heritage of Cincinnati for the second persona.')]), tool_calls=[ToolSelection(tool_id='personas', tool_name='personas', tool_kwargs={'input': 'science fiction'})], raw=Response(response=' It seems neither persona in the provided context has a particular interest or expertise in science fiction. They focus on Cypriot culture and history for the first persona and 19th-century American art and local cultural heritage of Cincinnati for the second persona.', source_nodes=[NodeWithScore(node=TextNode(id_='b791fbd0-393d-4cd4-bbf7-f4cceb94a641', embedding=None, metadata={'file_path': '/home/gan/Documents/GIT_COPY/Hugging_Face_Formation/Unit 2.2 The LlamaIndex framework /data/persona_1.txt', 'file_name': 'persona_1.txt', 'file_type': 'text/plain', 'file_size': 266, 'creation_date': '2025-06-20', 'last_modified_date': '2025-06-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e6e9b540-201c-4fc2-9c40-2d66c43fa70c', node_type='4', metadata={'file_path': '/home/gan/Documents/GIT_COPY/Hugging_Face_Formation/Unit 2.2 The LlamaIndex framework /data/persona_1.txt', 'file_name': 'persona_1.txt', 'file_type': 'text/plain', 'file_size': 266, 'creation_date': '2025-06-20', 'last_modified_date': '2025-06-20'}, hash='fa29696408be890c8b350df10a69f28c8ba7e3aa8a40292b85d2b55ef28715fe')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='An anthropologist or a cultural expert interested in the intricacies of Cypriot culture, history, and society, particularly someone who has spent considerable time researching and living in Cyprus to gain a deep understanding of its people, customs, and way of life.', mimetype='text/plain', start_char_idx=0, end_char_idx=266, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.4022852010693525), NodeWithScore(node=TextNode(id_='a4713d82-3a21-434a-9ab3-4ae56fae0a96', embedding=None, metadata={'file_path': '/home/gan/Documents/GIT_COPY/Hugging_Face_Formation/Unit 2.2 The LlamaIndex framework /data/persona_0.txt', 'file_name': 'persona_0.txt', 'file_type': 'text/plain', 'file_size': 132, 'creation_date': '2025-06-20', 'last_modified_date': '2025-06-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b67c7304-411d-47f6-ab17-9d03c38a5738', node_type='4', metadata={'file_path': '/home/gan/Documents/GIT_COPY/Hugging_Face_Formation/Unit 2.2 The LlamaIndex framework /data/persona_0.txt', 'file_name': 'persona_0.txt', 'file_type': 'text/plain', 'file_size': 132, 'creation_date': '2025-06-20', 'last_modified_date': '2025-06-20'}, hash='d1fc22e5671830fb0d35115742323cf472d30063b41cb7177dcbf4ad02222b79')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='A local art historian and museum professional interested in 19th-century American art and the local cultural heritage of Cincinnati.', mimetype='text/plain', start_char_idx=0, end_char_idx=132, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.38989740324625444)], metadata={'b791fbd0-393d-4cd4-bbf7-f4cceb94a641': {'file_path': '/home/gan/Documents/GIT_COPY/Hugging_Face_Formation/Unit 2.2 The LlamaIndex framework /data/persona_1.txt', 'file_name': 'persona_1.txt', 'file_type': 'text/plain', 'file_size': 266, 'creation_date': '2025-06-20', 'last_modified_date': '2025-06-20'}, 'a4713d82-3a21-434a-9ab3-4ae56fae0a96': {'file_path': '/home/gan/Documents/GIT_COPY/Hugging_Face_Formation/Unit 2.2 The LlamaIndex framework /data/persona_0.txt', 'file_name': 'persona_0.txt', 'file_type': 'text/plain', 'file_size': 132, 'creation_date': '2025-06-20', 'last_modified_date': '2025-06-20'}}), current_agent_name='Agent')"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# handler = query_engine_agent.run(    \"Search the database for 'science fiction' and return some persona descriptions.\")\n",
        "# Did not use the tool I dunno why. So I try with a more direct query\n",
        "\n",
        "\n",
        "handler = query_engine_agent.run( \"Use the tool to get all personas related to 'science fiction'\")\n",
        "async for ev in handler.stream_events():\n",
        "    if isinstance(ev, ToolCallResult):\n",
        "        print(\"\")\n",
        "        print(\"Called tool: \", ev.tool_name, ev.tool_kwargs, \"=>\", ev.tool_output)\n",
        "    elif isinstance(ev, AgentStream):  # showing the thought process\n",
        "        print(ev.delta, end=\"\", flush=True)\n",
        "\n",
        "resp = await handler\n",
        "resp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3DqLdGYyExS"
      },
      "source": [
        "## Creating multi-agent systems\n",
        "\n",
        "We can also create multi-agent systems by passing multiple agents to the `AgentWorkflow` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "aVgPDPNGyExT"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent.workflow import (\n",
        "    AgentWorkflow,\n",
        "    ReActAgent,\n",
        ")\n",
        "\n",
        "\n",
        "# Define some tools\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two numbers.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "def subtract(a: int, b: int) -> int:\n",
        "    \"\"\"Subtract two numbers.\"\"\"\n",
        "    return a - b\n",
        "\n",
        "\n",
        "# Create agent configs\n",
        "# NOTE: we can use FunctionAgent or ReActAgent here.\n",
        "# FunctionAgent works for LLMs with a function calling API.\n",
        "# ReActAgent works for any LLM.\n",
        "calculator_agent = ReActAgent(\n",
        "    name=\"calculator\",\n",
        "    description=\"Performs basic arithmetic operations\",\n",
        "    system_prompt=\"You are a calculator assistant. Use your tools for any math operation.\",\n",
        "    tools=[add, subtract],\n",
        "    llm=llm,\n",
        ")\n",
        "\n",
        "query_agent = ReActAgent(\n",
        "    name=\"info_lookup\",\n",
        "    description=\"Looks up information about XYZ\",\n",
        "    system_prompt=\"Use your tool to query a RAG system to answer information about XYZ\",\n",
        "    tools=[query_engine_tool],\n",
        "    llm=llm,\n",
        ")\n",
        "\n",
        "# Create and run the workflow\n",
        "agent = AgentWorkflow(agents=[calculator_agent, query_agent], root_agent=\"calculator\")\n",
        "\n",
        "# Run the system\n",
        "handler = agent.run(user_msg=\"Can you add 5 and 3?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Wg8g_CtCyExT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Thought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
            "Action: add\n",
            "Action Input: {\"a\": 5, \"b\": 3}\n",
            "\n",
            "Observation: 8\n",
            "\n",
            "Thought: I can answer without using any more tools. I'll use the user's language to answer.\n",
            "Answer: The sum of 5 and 3 is 8.\n",
            "Called tool:  add {'a': 5, 'b': 3} => 8\n",
            " Thought: The user has provided a number, 8. I need to use a tool to answer their question if applicable.\n",
            "Action: None\n",
            "Answer: The number you provided is 8. Is there anything specific you would like me to do with this number or any other number?"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AgentOutput(response=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={'tool_calls': [], 'thinking': ''}, blocks=[TextBlock(block_type='text', text=' Thought: The user has provided a number, 8. I need to use a tool to answer their question if applicable.\\nAction: None\\nAnswer: The number you provided is 8. Is there anything specific you would like me to do with this number or any other number?')]), tool_calls=[ToolCallResult(tool_name='add', tool_kwargs={'a': 5, 'b': 3}, tool_id='6ebe43ce-73a3-4506-b022-72c62abb2dad', tool_output=ToolOutput(content='8', tool_name='add', raw_input={'args': (), 'kwargs': {'a': 5, 'b': 3}}, raw_output=8, is_error=False), return_direct=False)], raw={'model': 'mistral:instruct', 'created_at': '2025-06-20T10:44:58.414725352Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3001906966, 'load_duration': 11180997, 'prompt_eval_count': 888, 'prompt_eval_duration': 964199009, 'eval_count': 60, 'eval_duration': 2014196021, 'message': Message(role='assistant', content='', thinking=None, images=None, tool_calls=None), 'usage': {'prompt_tokens': 888, 'completion_tokens': 60, 'total_tokens': 948}}, current_agent_name='calculator')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "async for ev in handler.stream_events():\n",
        "    if isinstance(ev, ToolCallResult):\n",
        "        print(\"\")\n",
        "        print(\"Called tool: \", ev.tool_name, ev.tool_kwargs, \"=>\", ev.tool_output)\n",
        "    elif isinstance(ev, AgentStream):  # showing the thought process\n",
        "        print(ev.delta, end=\"\", flush=True)\n",
        "\n",
        "resp = await handler\n",
        "resp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observations\n",
        "\n",
        "Les differents modeles de Ollama ne sont pas tous adaptes a ce genre d'utilisation.\n",
        "\n",
        "Le modele utilise ici est \n",
        "> mistral:instruct\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
