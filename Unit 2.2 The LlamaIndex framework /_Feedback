## ğŸ’¬ Feedback on Unit 2.2 â€” LlamaIndex

### ğŸ”„ Switching to Local Models
- Easier to understand how to replace inference models with local ones.
- More flexibility in LLM configuration compared to SmolAgents.


### âš ï¸ Model Compatibility
- Not all models work well across tasks (e.g. agents, query engines).
- A compatibility guide would help.

### ğŸ’¥ Heavy Workflows
- Agentic workflows are **resource-intensive**, even for simple tasks.
- High compute usage and latency with multi-agent setups.



---

## ğŸ”š Conclusion
- LlamaIndex is flexible and modular, with better local model support than SmolAgents.
- Needs lighter workflows and clearer guidance for local usage.
