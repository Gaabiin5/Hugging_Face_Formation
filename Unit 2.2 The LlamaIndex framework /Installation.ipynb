{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90624a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install llama-index-{component-type}-{framework-name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ac17001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-llms-huggingface-api\n",
      "  Downloading llama_index_llms_huggingface_api-0.5.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting llama-index-embeddings-huggingface\n",
      "  Downloading llama_index_embeddings_huggingface-0.5.4-py3-none-any.whl.metadata (458 bytes)\n",
      "Requirement already satisfied: huggingface-hub>=0.31.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-llms-huggingface-api) (0.33.0)\n",
      "Collecting llama-index-core<0.13,>=0.12.0 (from llama-index-llms-huggingface-api)\n",
      "  Downloading llama_index_core-0.12.43-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting sentence-transformers>=2.6.1 (from llama-index-embeddings-huggingface)\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /home/gan/venv/lib/python3.12/site-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/gan/venv/lib/python3.12/site-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (2025.5.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/gan/venv/lib/python3.12/site-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/gan/venv/lib/python3.12/site-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/gan/venv/lib/python3.12/site-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/gan/venv/lib/python3.12/site-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/gan/venv/lib/python3.12/site-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/gan/venv/lib/python3.12/site-packages (from huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (1.1.4)\n",
      "Requirement already satisfied: aiohttp in /home/gan/venv/lib/python3.12/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.12.13)\n",
      "Requirement already satisfied: aiosqlite in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (0.21.0)\n",
      "Collecting banks<3,>=2.0.0 (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading banks-2.1.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: dataclasses-json in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (0.6.7)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2,>=1.2.0 (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: httpx in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (0.28.1)\n",
      "Collecting llama-index-workflows>=0.2.1 (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading llama_index_workflows-0.2.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (1.6.0)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting nltk>3.8.1 (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (2.3.0)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (11.2.1)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (2.11.7)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in /home/gan/venv/lib/python3.12/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (2.0.41)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (0.9.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /home/gan/venv/lib/python3.12/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (1.17.2)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: scikit-learn in /home/gan/venv/lib/python3.12/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.7.0)\n",
      "Requirement already satisfied: scipy in /home/gan/venv/lib/python3.12/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.15.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/gan/venv/lib/python3.12/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/gan/venv/lib/python3.12/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/gan/venv/lib/python3.12/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/gan/venv/lib/python3.12/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/gan/venv/lib/python3.12/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/gan/venv/lib/python3.12/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/gan/venv/lib/python3.12/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.20.1)\n",
      "Collecting griffe (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading griffe-1.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: jinja2 in /home/gan/venv/lib/python3.12/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (3.1.6)\n",
      "Requirement already satisfied: platformdirs in /home/gan/venv/lib/python3.12/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (4.3.8)\n",
      "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows>=0.2.1->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading llama_index_instrumentation-0.1.0-py3-none-any.whl.metadata (252 bytes)\n",
      "Requirement already satisfied: click in /home/gan/venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/gan/venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/gan/venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/gan/venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/gan/venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/gan/venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/gan/venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gan/venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gan/venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gan/venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.31.0->llama-index-llms-huggingface-api) (2025.6.15)\n",
      "Requirement already satisfied: greenlet>=1 in /home/gan/venv/lib/python3.12/site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (3.2.3)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.1 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/gan/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.21.1)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/gan/venv/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/gan/venv/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (3.26.1)\n",
      "Requirement already satisfied: anyio in /home/gan/venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/gan/venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/gan/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (0.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/gan/venv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.6.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/gan/venv/lib/python3.12/site-packages (from anyio->httpx->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (1.3.1)\n",
      "Collecting colorama>=0.4 (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/gan/venv/lib/python3.12/site-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-llms-huggingface-api) (3.0.2)\n",
      "Downloading llama_index_llms_huggingface_api-0.5.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading llama_index_embeddings_huggingface-0.5.4-py3-none-any.whl (8.9 kB)\n",
      "Downloading llama_index_core-0.12.43-py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.7/345.7 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading banks-2.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading llama_index_workflows-0.2.2-py3-none-any.whl (33 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl (821.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.0/821.0 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_instrumentation-0.1.0-py3-none-any.whl (14 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading griffe-1.7.3-py3-none-any.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, mpmath, filetype, dirtyjson, triton, sympy, safetensors, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nltk, networkx, deprecated, colorama, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, griffe, nvidia-cusolver-cu12, llama-index-instrumentation, banks, transformers, torch, llama-index-workflows, sentence-transformers, llama-index-core, llama-index-llms-huggingface-api, llama-index-embeddings-huggingface\n",
      "Successfully installed banks-2.1.2 colorama-0.4.6 deprecated-1.2.18 dirtyjson-1.0.8 filetype-1.2.0 griffe-1.7.3 llama-index-core-0.12.43 llama-index-embeddings-huggingface-0.5.4 llama-index-instrumentation-0.1.0 llama-index-llms-huggingface-api-0.5.0 llama-index-workflows-0.2.2 mpmath-1.3.0 networkx-3.5 nltk-3.9.1 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 safetensors-0.5.3 sentence-transformers-4.1.0 sympy-1.14.0 torch-2.7.1 transformers-4.52.4 triton-3.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-llms-huggingface-api llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d82ba4b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions (Request ID: Root=1-68541441-47c68fef548f533f13894e95;99b629d0-302c-4e13-bea0-1326410dacf7)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      9\u001b[39m hf_token = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mHF_TOKEN\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m llm = HuggingFaceInferenceAPI(\n\u001b[32m     12\u001b[39m     model_name=\u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen2.5-Coder-32B-Instruct\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m     temperature=\u001b[32m0.7\u001b[39m,\n\u001b[32m     14\u001b[39m     max_tokens=\u001b[32m100\u001b[39m,\n\u001b[32m     15\u001b[39m     token=hf_token,\n\u001b[32m     16\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHello, how are you?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# I am good, how can I help you today?\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py:319\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    316\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    322\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/llama_index/llms/huggingface_api/base.py:314\u001b[39m, in \u001b[36mHuggingFaceInferenceAPI.complete\u001b[39m\u001b[34m(self, prompt, formatted, **kwargs)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcomplete\u001b[39m(\n\u001b[32m    311\u001b[39m     \u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, formatted: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs: Any\n\u001b[32m    312\u001b[39m ) -> CompletionResponse:\n\u001b[32m    313\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.task == \u001b[33m\"\u001b[39m\u001b[33mconversational\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         chat_resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrole\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMessageRole\u001b[49m\u001b[43m.\u001b[49m\u001b[43mUSER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m chat_response_to_completion_response(chat_resp)\n\u001b[32m    319\u001b[39m     model_kwargs = \u001b[38;5;28mself\u001b[39m._get_model_kwargs(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py:319\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    316\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    322\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/llama_index/llms/huggingface_api/base.py:287\u001b[39m, in \u001b[36mHuggingFaceInferenceAPI.chat\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.task == \u001b[33m\"\u001b[39m\u001b[33mconversational\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    285\u001b[39m     model_kwargs = \u001b[38;5;28mself\u001b[39m._get_model_kwargs(**kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     output: ChatCompletionOutput = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sync_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_to_huggingface_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     content = output.choices[\u001b[32m0\u001b[39m].message.content \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    293\u001b[39m     tool_calls = output.choices[\u001b[32m0\u001b[39m].message.tool_calls \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:924\u001b[39m, in \u001b[36mInferenceClient.chat_completion\u001b[39m\u001b[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[39m\n\u001b[32m    896\u001b[39m parameters = {\n\u001b[32m    897\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: payload_model,\n\u001b[32m    898\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   (...)\u001b[39m\u001b[32m    915\u001b[39m     **(extra_body \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    916\u001b[39m }\n\u001b[32m    917\u001b[39m request_parameters = provider_helper.prepare_request(\n\u001b[32m    918\u001b[39m     inputs=messages,\n\u001b[32m    919\u001b[39m     parameters=parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.token,\n\u001b[32m    923\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m924\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:280\u001b[39m, in \u001b[36mInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    277\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response.content\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:482\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions (Request ID: Root=1-68541441-47c68fef548f533f13894e95;99b629d0-302c-4e13-bea0-1326410dacf7)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve HF_TOKEN from the environment variables\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    "    token=hf_token,\n",
    ")\n",
    "\n",
    "response = llm.complete(\"Hello, how are you?\")\n",
    "print(response)\n",
    "# I am good, how can I help you today?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
